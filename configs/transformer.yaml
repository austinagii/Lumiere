model:
  vocab_size: 4096
  context_size: 64
  embedding_size: 128
  num_blocks: 4
  embedding_factory:
    type: sinusoidal
    padding_id: 0
  block_factory:
    type: standard
    attention_factory:
      type: multihead
      num_heads: 4
      d_key: 32
      d_value: 32
    feedforward_factory:
      type: linear
      d_ff: 256
    normalization_factory:
      type: rms
      normalized_shape: 128
    dropout: 0.1
    pre_norm: True
    post_norm: False
  normalization_factory:
    type: rms
    normalized_shape: 128

tokenizer:
  type: bpe
  vocab_size: 4096
  min_frequency: 2

data:
  datasets:
    - name: wikitext

training:
  batch_size: 32
  sliding_window_size: 8
  learning_rate: 0.0003
  weight_decay: 0.01
  gradient_clip_norm: 1.0
  dropout: 0.1
  max_epochs: 250
  epoch_steps: 2000
  warmup_steps: 500
  patience: 5
  stopping_threshold: 0.0001
  checkpoint_interval: 3

logging:
  interval: 50
