tokenizer:
  vocab_size: 32000
  batch_size: 128
  output_path: "tokenizers/base_large"