tokenizer:
  vocab_size: 32768
  batch_size: 1024