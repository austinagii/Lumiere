model:
  vocab_size: 4096
  context_size: 64
  embedding_size: 128
  num_blocks: 4
  embedding_factory:
    type: sinusoidal
    padding_id: 0
  block_factory:
    type: standard
    attention_factory:
      type: multihead
      num_heads: 4
      d_key: 32
      d_value: 32
    feedforward_factory:
      type: linear
      d_ff: 256
    normalization_factory:
      type: rms
      normalized_shape: 128
    dropout: 0.1
    pre_norm: True
    post_norm: False
  normalization_factory:
    type: rms
    normalized_shape: 128

data:
  datasets:
    - name: wikitext
  pipeline:
    type: text
    batch_size: 32
    context_size: 64
    pad_id: -1
    sliding_window_size: 8
    preprocessors:
      - autoregressive

tokenizer:
  type: bpe
  vocab_size: 4096
  min_frequency: 2

optimizer:
  type: adamw
  lr: 0.0003
  weight_decay: 0.01

scheduler:
  type: cosine-annealing
  warmup_steps: 500
  max_epochs: 250
  epoch_steps: 2000

training:
  max_epochs: 250
  stopping_threshold: 0.0001
  patience: 5
  gradient_clip_norm: 1.0

logging:
  interval: 50
