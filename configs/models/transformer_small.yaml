# Small Base Configuration
model:
  architecture: "base"
  tokenizer: "bpe_small"
  embedding_size: 256
  context_size: 256
  num_layers: 6
  num_heads: 12
  d_key: 64
  d_value: 64
  d_ff: 1024
  dropout: 0.1
  output_path: "artifacts/models/transformer_small"

training:
  batch_size: 64
  learning_rate: 0.0003
  weight_decay: 0.01
  gradient_clip_norm: 1.0
  num_epochs: 10
  warmup_steps: 1000
  patience: 5