# Large Transformer Configuration
model:
  tokenizer: "bpe-large"
  embedding_size: 384
  context_size: 256
  num_layers: 12
  num_heads: 12
  d_key: 128
  d_value: 128
  d_ff: 2048
  dropout: 0.1

training:
  batch_size: 128 
  learning_rate: 0.0003
  weight_decay: 0.01
  gradient_clip_norm: 1.0
  max_epochs: 300
  epoch_steps: 60
  warmup_steps: 120
  patience: 20
  stopping_threshold: 0.0001